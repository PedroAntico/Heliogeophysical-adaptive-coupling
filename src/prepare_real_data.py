#!/usr/bin/env python3
"""
prepare_real_data.py - versÃ£o corrigida
CompatÃ­vel com o arquivo omni_labeled.csv
"""

import os
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import numpy as np

INPUT_FILE = "data_real/omni_labeled.csv"
OUTPUT_DATA = "data_real/omni_scaled.npy"
OUTPUT_SCALERS = "data_real/scalers.pkl"

# Mapeamento correto com base no rename_omni_columns.py
RENAME_MAP = {
    "Speed": "speed",
    "Density": "density",
    "Bz_GSE": "bz",
    "Pressure": "pressure",
    "Bt": "bt"
}

FEATURES = ["speed", "density", "bz", "pressure", "bt"]


def load_and_prepare():
    print("ğŸ“‚ Lendo dados:", INPUT_FILE)
    df = pd.read_csv(INPUT_FILE)

    print("ğŸ”§ Renomeando colunas para padrÃ£o HAC...")
    df = df.rename(columns=RENAME_MAP)

    print("ğŸ“Š Colunas atuais:", df.columns.tolist())

    # VerificaÃ§Ã£o
    for feat in FEATURES:
        if feat not in df.columns:
            raise ValueError(f"âŒ ERRO: coluna '{feat}' nÃ£o encontrada no CSV!")

    print("âœ”ï¸ Todas as colunas essenciais encontradas!")

    return df


def scale_data(df):
    print("ğŸ”§ Normalizando dados...")
    scalers = {}
    scaled = pd.DataFrame()

    for col in FEATURES:
        sc = MinMaxScaler()
        scaled[col] = sc.fit_transform(df[col].values.reshape(-1, 1)).flatten()
        scalers[col] = sc

    print("âœ”ï¸ NormalizaÃ§Ã£o concluÃ­da!")
    return scaled, scalers


def save_outputs(scaled, scalers):
    import pickle

    print("ğŸ’¾ Salvando dados normalizados...")
    np.save(OUTPUT_DATA, scaled.values)

    print("ğŸ’¾ Salvando scalers...")
    with open(OUTPUT_SCALERS, "wb") as f:
        pickle.dump(scalers, f)

    print("ğŸ‰ Arquivos salvos:")
    print(" â€¢", OUTPUT_DATA)
    print(" â€¢", OUTPUT_SCALERS)


def main():
    df = load_and_prepare()
    scaled, scalers = scale_data(df)
    save_outputs(scaled, scalers)

    print("\nğŸ¯ TUDO PRONTO!")
    print("Agora vocÃª jÃ¡ pode treinar o HAC modelo real-time.")


if __name__ == "__main__":
    main()
