#!/usr/bin/env python3
"""
train_hac_omni_multitarget.py - HAC 6.0
Treino Deep Learning Multi-Horizonte e Multi-Alvo usando dados OMNI reais

Melhorias implementadas:
1. Sistema robusto de detec√ß√£o e tratamento de dados
2. Feature engineering com m√©dias m√≥veis e diferencia√ß√£o
3. Valida√ß√£o cruzada temporal
4. Otimiza√ß√£o de hiperpar√¢metros com KerasTuner
5. Ensemble learning
6. Sistema de checkpoint inteligente
7. M√©tricas avan√ßadas e an√°lise de res√≠duos
8. Visualiza√ß√µes integradas
9. Pipeline modularizado
10. Tratamento de dados desbalanceados
"""

import os
import json
import logging
import warnings
import traceback
from datetime import datetime, timedelta
from typing import List, Dict, Any, Tuple, Optional, Union

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import RobustScaler, StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import TimeSeriesSplit

import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import (LSTM, GRU, Dense, Dropout, BatchNormalization, 
                                   Input, Concatenate, Attention, MultiHeadAttention,
                                   Conv1D, MaxPooling1D, Flatten, GlobalAveragePooling1D)
from tensorflow.keras.optimizers import Adam, RMSprop
from tensorflow.keras.callbacks import (EarlyStopping, ReduceLROnPlateau, ModelCheckpoint,
                                      TensorBoard, CSVLogger)
from tensorflow.keras.regularizers import l1_l2
from tensorflow.keras.utils import plot_model
import keras_tuner as kt

# ============================================================
# CONFIGURA√á√ÉO AVAN√áADA
# ============================================================

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - HAC_OMNI_V6 - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(f"hac_train_omni_v6_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.log", 
                          encoding="utf-8")
    ]
)
logger = logging.getLogger("HAC_OMNI_V6")

warnings.filterwarnings("ignore")
tf.get_logger().setLevel("ERROR")
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"
os.environ["TF_ENABLE_ONEDNN_OPTS"] = "0"

# Configura√ß√µes avan√ßadas
class AdvancedTrainConfig:
    # Per√≠odos de lookback m√∫ltiplos para capturar diferentes escalas temporais
    LOOKBACK_WINDOWS = [24, 72, 168]  # 1 dia, 3 dias, 1 semana
    MAIN_LOOKBACK = 168
    
    # Arquitetura avan√ßada
    BATCH_SIZE = 64
    MAX_EPOCHS = 150
    VAL_SPLIT = 0.15
    TEST_SPLIT = 0.15
    LEARNING_RATE = 1e-3
    DROPOUT_RATES = [0.2, 0.3, 0.4]  # Para diferentes camadas
    
    # Engenharia de features
    ROLLING_WINDOWS = [6, 12, 24, 48]  # Para m√©dias m√≥veis
    USE_FEATURE_ENGINEERING = True
    USE_SEASONAL_FEATURES = True
    
    # Valida√ß√£o
    N_SPLITS = 5  # Para TimeSeriesSplit
    USE_CROSS_VALIDATION = True
    
    # Hiperpar√¢metros
    TUNE_HYPERPARAMETERS = True
    MAX_TRIALS = 20
    
    # Ensemble
    USE_ENSEMBLE = True
    N_ENSEMBLE_MODELS = 3
    
    # M√≠nimos
    MIN_SAMPLES = 500
    MIN_SAMPLES_RATIO = 0.01

# ============================================================
# OTIMIZA√á√ÉO DE GPU E PERFORMANCE
# ============================================================

def setup_advanced_gpu():
    """Configura√ß√£o otimizada para GPU"""
    gpus = tf.config.experimental.list_physical_devices("GPU")
    if gpus:
        try:
            # Configura√ß√£o de mem√≥ria din√¢mica
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            
            # Estrat√©gia de distribui√ß√£o para m√∫ltiplas GPUs
            if len(gpus) > 1:
                strategy = tf.distribute.MirroredStrategy()
                logger.info(f"‚úÖ {len(gpus)} GPUs detectadas - Estrat√©gia Mirrored ativada")
                return strategy
            else:
                strategy = tf.distribute.OneDeviceStrategy(device="/gpu:0")
                logger.info("‚úÖ 1 GPU detectada - Configura√ß√£o otimizada")
                return strategy
                
        except RuntimeError as e:
            logger.warning(f"‚ö†Ô∏è Erro ao configurar GPU: {e}")
    
    logger.info("üî∂ Treinando em CPU")
    return tf.distribute.OneDeviceStrategy(device="/cpu:0")

# ============================================================
# CARREGAMENTO E VALIDA√á√ÉO DE DADOS AVAN√áADA
# ============================================================

def load_and_validate_omni_data() -> pd.DataFrame:
    """
    Carrega e valida dados OMNI com verifica√ß√µes avan√ßadas
    """
    possible_paths = [
        "data_real/omni_prepared.csv",
        "data_real/omni_converted.csv", 
        "data_real/omni_processed.csv",
        "data/omni_dataset.csv"
    ]
    
    df = None
    for path in possible_paths:
        if os.path.exists(path):
            logger.info(f"üìÇ Carregando dados de: {path}")
            try:
                df = pd.read_csv(path)
                # Verificar se tem colunas suficientes
                if len(df.columns) >= 3 and len(df) > 1000:
                    logger.info(f"‚úÖ Arquivo v√°lido: {len(df)} linhas, {len(df.columns)} colunas")
                    break
                else:
                    logger.warning(f"‚ö†Ô∏è Arquivo muito pequeno ou poucas colunas: {path}")
                    df = None
            except Exception as e:
                logger.error(f"‚ùå Erro ao ler {path}: {e}")
                continue
    
    if df is None:
        # Criar dataset de exemplo para testes
        logger.warning("üö® Nenhum arquivo encontrado - Criando dataset de exemplo")
        df = create_sample_omni_data()
    
    return validate_and_preprocess_data(df)

def create_sample_omni_data() -> pd.DataFrame:
    """Cria dados OMNI de exemplo para desenvolvimento"""
    dates = pd.date_range(start='2010-01-01', end='2020-12-31', freq='H')
    n_samples = len(dates)
    
    # Gerar dados realistas com sazonalidade e tend√™ncias
    np.random.seed(42)
    
    data = {
        'timestamp': dates,
        'speed': 300 + 100 * np.sin(2 * np.pi * np.arange(n_samples) / 8760) + 50 * np.random.normal(size=n_samples),
        'bz_gse': 2 * np.sin(2 * np.pi * np.arange(n_samples) / 24) + 5 * np.random.normal(size=n_samples),
        'density': 5 + 2 * np.sin(2 * np.pi * np.arange(n_samples) / 8760) + np.random.normal(size=n_samples),
        'bt': 6 + 2 * np.random.normal(size=n_samples),
        'temperature': 1e5 + 2e4 * np.random.normal(size=n_samples),
        'pressure': 2 + 0.5 * np.random.normal(size=n_samples),
    }
    
    df = pd.DataFrame(data)
    df['speed'] = np.abs(df['speed'])  # Velocidade sempre positiva
    df['density'] = np.abs(df['density'])  # Densidade sempre positiva
    
    logger.info("üìù Dataset de exemplo criado para desenvolvimento")
    return df

def validate_and_preprocess_data(df: pd.DataFrame) -> pd.DataFrame:
    """
    Valida e pr√©-processa dados com verifica√ß√µes avan√ßadas
    """
    logger.info("üîç Validando e pr√©-processando dados...")
    
    # Verificar timestamp
    if 'timestamp' not in df.columns:
        raise ValueError("‚ùå Coluna 'timestamp' n√£o encontrada")
    
    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')
    df = df.dropna(subset=['timestamp']).sort_values('timestamp').reset_index(drop=True)
    
    # Verificar continuidade temporal
    time_diff = df['timestamp'].diff().dt.total_seconds().div(3600)
    gaps = time_diff[time_diff > 1].count()
    if gaps > 0:
        logger.warning(f"‚ö†Ô∏è {gaps} lacunas temporais detectadas (>1 hora)")
    
    # Verificar valores num√©ricos
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    logger.info(f"üìä Colunas num√©ricas: {list(numeric_cols)}")
    
    # Estat√≠sticas b√°sicas
    for col in numeric_cols:
        n_zeros = (df[col] == 0).sum()
        n_negative = (df[col] < 0).sum() if col not in ['speed', 'density'] else 0
        n_missing = df[col].isna().sum()
        
        logger.info(f"   {col:15s}: {len(df)-n_missing:5d} v√°lidos, "
                   f"{n_missing:4d} missing, {n_zeros:4d} zeros, {n_negative:4d} negativos")
    
    # Remover colunas com muitos missing values
    threshold = 0.3 * len(df)
    valid_cols = [col for col in numeric_cols if df[col].count() > threshold]
    df = df[['timestamp'] + valid_cols]
    
    logger.info(f"‚úÖ Dados validados: {len(df)} linhas, {len(valid_cols)} colunas num√©ricas")
    return df

# ============================================================
# ENGENHARIA DE FEATURES AVAN√áADA
# ============================================================

class AdvancedFeatureEngineer:
    """Classe para engenharia avan√ßada de features"""
    
    @staticmethod
    def add_temporal_features(df: pd.DataFrame) -> pd.DataFrame:
        """Adiciona features temporais"""
        df = df.copy()
        dt = pd.to_datetime(df['timestamp'])
        
        # Features c√≠clicas
        df['hour_sin'] = np.sin(2 * np.pi * dt.dt.hour / 24)
        df['hour_cos'] = np.cos(2 * np.pi * dt.dt.hour / 24)
        df['doy_sin'] = np.sin(2 * np.pi * dt.dt.dayofyear / 365)
        df['doy_cos'] = np.cos(2 * np.pi * dt.dt.dayofyear / 365)
        
        # Features sazonais
        df['month'] = dt.dt.month
        df['day_of_week'] = dt.dt.dayofweek
        df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)
        
        return df
    
    @staticmethod
    def add_rolling_features(df: pd.DataFrame, columns: List[str]) -> pd.DataFrame:
        """Adiciona m√©dias m√≥veis e estat√≠sticas rolling"""
        df = df.copy()
        
        for col in columns:
            for window in AdvancedTrainConfig.ROLLING_WINDOWS:
                # M√©dias m√≥veis
                df[f'{col}_rolling_mean_{window}'] = df[col].rolling(
                    window=window, min_periods=1, center=True
                ).mean()
                
                # Desvios padr√£o
                df[f'{col}_rolling_std_{window}'] = df[col].rolling(
                    window=window, min_periods=1, center=True
                ).std()
                
                # M√≠nimos e m√°ximos
                df[f'{col}_rolling_min_{window}'] = df[col].rolling(
                    window=window, min_periods=1, center=True
                ).min()
                df[f'{col}_rolling_max_{window}'] = df[col].rolling(
                    window=window, min_periods=1, center=True
                ).max()
        
        return df
    
    @staticmethod
    def add_lag_features(df: pd.DataFrame, columns: List[str], lags: List[int]) -> pd.DataFrame:
        """Adiciona features defasadas"""
        df = df.copy()
        
        for col in columns:
            for lag in lags:
                df[f'{col}_lag_{lag}'] = df[col].shift(lag)
        
        return df
    
    @staticmethod
    def add_physical_features(df: pd.DataFrame) -> pd.DataFrame:
        """Adiciona features f√≠sicas derivadas"""
        df = df.copy()
        
        # Press√£o din√¢mica (œÅv¬≤)
        if 'density' in df.columns and 'speed' in df.columns:
            df['dynamic_pressure'] = df['density'] * df['speed'] ** 2 * 1.6726e-6
        
        # Beta plasm√°tico (press√£o t√©rmica / press√£o magn√©tica)
        if 'temperature' in df.columns and 'density' in df.columns and 'bt' in df.columns:
            thermal_pressure = 2 * 1.38e-23 * df['temperature'] * df['density'] * 1e6
            magnetic_pressure = (df['bt'] * 1e-9) ** 2 / (2 * 4e-7 * np.pi)
            df['plasma_beta'] = thermal_pressure / magnetic_pressure
        
        return df

def create_advanced_features(df: pd.DataFrame, target_cols: List[str]) -> pd.DataFrame:
    """Pipeline completo de engenharia de features"""
    logger.info("üîß Aplicando engenharia avan√ßada de features...")
    
    engineer = AdvancedFeatureEngineer()
    
    # Features temporais
    if AdvancedTrainConfig.USE_SEASONAL_FEATURES:
        df = engineer.add_temporal_features(df)
    
    # Features rolling (excluindo targets para evitar data leakage)
    feature_cols = [col for col in df.select_dtypes(include=[np.number]).columns 
                   if col not in target_cols and col != 'timestamp']
    df = engineer.add_rolling_features(df, feature_cols)
    
    # Features defasadas
    lags = [1, 2, 3, 6, 12, 24]
    df = engineer.add_lag_features(df, feature_cols, lags)
    
    # Features f√≠sicas
    df = engineer.add_physical_features(df)
    
    # Preencher valores NaN resultantes
    df = df.fillna(method='bfill').fillna(method='ffill')
    
    logger.info(f"üéØ Engenharia de features completa: {len(df.columns)} colunas totais")
    return df

# ============================================================
# MODELOS AVAN√áADOS
# ============================================================

def build_hybrid_model(input_shape: Tuple[int, int], output_dim: int) -> Model:
    """
    Modelo h√≠brido CNN-LSTM com aten√ß√£o
    """
    inputs = Input(shape=input_shape)
    
    # Branch CNN para features locais
    conv1 = Conv1D(64, kernel_size=3, activation='relu', padding='same')(inputs)
    conv1 = BatchNormalization()(conv1)
    conv1 = MaxPooling1D(pool_size=2)(conv1)
    
    conv2 = Conv1D(128, kernel_size=3, activation='relu', padding='same')(conv1)
    conv2 = BatchNormalization()(conv2)
    conv2 = MaxPooling1D(pool_size=2)(conv2)
    
    # Branch LSTM para depend√™ncias temporais
    lstm1 = LSTM(128, return_sequences=True, dropout=0.2)(inputs)
    lstm1 = BatchNormalization()(lstm1)
    
    lstm2 = LSTM(64, return_sequences=True, dropout=0.2)(lstm1)
    lstm2 = BatchNormalization()(lstm2)
    
    # Aten√ß√£o
    attention = MultiHeadAttention(num_heads=4, key_dim=64)(lstm2, lstm2)
    
    # Combina√ß√£o
    cnn_flat = GlobalAveragePooling1D()(conv2)
    lstm_flat = GlobalAveragePooling1D()(attention)
    
    combined = Concatenate()([cnn_flat, lstm_flat])
    
    # Camadas densas
    dense1 = Dense(128, activation='relu', kernel_regularizer=l1_l2(0.01, 0.01))(combined)
    dense1 = BatchNormalization()(dense1)
    dense1 = Dropout(0.3)(dense1)
    
    dense2 = Dense(64, activation='relu', kernel_regularizer=l1_l2(0.01, 0.01))(dense1)
    dense2 = Dropout(0.3)(dense2)
    
    outputs = Dense(output_dim)(dense2)
    
    model = Model(inputs=inputs, outputs=outputs)
    model.compile(
        optimizer=Adam(learning_rate=AdvancedTrainConfig.LEARNING_RATE),
        loss='mse',
        metrics=['mae', 'mse']
    )
    
    return model

def build_attention_lstm(input_shape: Tuple[int, int], output_dim: int) -> Model:
    """LSTM com mecanismo de aten√ß√£o"""
    inputs = Input(shape=input_shape)
    
    lstm1 = LSTM(128, return_sequences=True, dropout=0.3)(inputs)
    lstm1 = BatchNormalization()(lstm1)
    
    lstm2 = LSTM(64, return_sequences=True, dropout=0.3)(lstm1)
    lstm2 = BatchNormalization()(lstm2)
    
    # Mecanismo de aten√ß√£o
    attention = Dense(1, activation='tanh')(lstm2)
    attention = tf.nn.softmax(attention, axis=1)
    context = tf.reduce_sum(attention * lstm2, axis=1)
    
    dense1 = Dense(64, activation='relu')(context)
    dense1 = Dropout(0.3)(dense1)
    
    dense2 = Dense(32, activation='relu')(dense1)
    outputs = Dense(output_dim)(dense2)
    
    model = Model(inputs=inputs, outputs=outputs)
    model.compile(
        optimizer=Adam(learning_rate=AdvancedTrainConfig.LEARNING_RATE),
        loss='mse',
        metrics=['mae']
    )
    
    return model

# ============================================================
# OTIMIZA√á√ÉO DE HIPERPAR√ÇMETROS
# ============================================================

class HACHyperModel(kt.HyperModel):
    def __init__(self, input_shape, output_dim):
        self.input_shape = input_shape
        self.output_dim = output_dim
    
    def build(self, hp):
        model_type = hp.Choice('model_type', ['lstm', 'gru', 'hybrid'])
        
        if model_type == 'hybrid':
            model = build_hybrid_model(self.input_shape, self.output_dim)
        else:
            # Configur√°vel LSTM/GRU
            n_layers = hp.Int('n_layers', 2, 4)
            units = hp.Int('units', 64, 256, step=64)
            dropout = hp.Float('dropout', 0.1, 0.5, step=0.1)
            
            inputs = Input(shape=self.input_shape)
            x = inputs
            
            for i in range(n_layers):
                return_sequences = i < n_layers - 1
                if model_type == 'lstm':
                    x = LSTM(units, return_sequences=return_sequences, 
                            dropout=dropout)(x)
                else:
                    x = GRU(units, return_sequences=return_sequences,
                           dropout=dropout)(x)
                x = BatchNormalization()(x)
            
            x = Dense(hp.Int('dense_units', 32, 128, step=32), 
                     activation='relu')(x)
            x = Dropout(dropout)(x)
            outputs = Dense(self.output_dim)(x)
            
            model = Model(inputs=inputs, outputs=outputs)
        
        learning_rate = hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')
        model.compile(
            optimizer=Adam(learning_rate=learning_rate),
            loss='mse',
            metrics=['mae']
        )
        
        return model

# ============================================================
# VALIDA√á√ÉO CRUZADA TEMPORAL
# ============================================================

def temporal_cross_validation(model_builder, X, y, n_splits=5):
    """Executa valida√ß√£o cruzada temporal"""
    tscv = TimeSeriesSplit(n_splits=n_splits)
    scores = []
    
    for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):
        logger.info(f"üîÅ Fold {fold + 1}/{n_splits}")
        
        X_train, X_val = X[train_idx], X[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]
        
        model = model_builder()
        history = model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=50,
            batch_size=AdvancedTrainConfig.BATCH_SIZE,
            verbose=0,
            callbacks=[
                EarlyStopping(patience=10, restore_best_weights=True)
            ]
        )
        
        # Avaliar
        val_loss = model.evaluate(X_val, y_val, verbose=0)[0]
        scores.append(val_loss)
        
        logger.info(f"   Fold {fold + 1} - Val Loss: {val_loss:.4f}")
    
    return np.mean(scores), np.std(scores)

# ============================================================
# M√âTRICAS AVAN√áADAS E AN√ÅLISE
# ============================================================

class AdvancedMetrics:
    """Classe para m√©tricas avan√ßadas e an√°lise"""
    
    @staticmethod
    def compute_comprehensive_metrics(y_true: np.ndarray, y_pred: np.ndarray, 
                                    target_names: List[str]) -> Dict[str, Any]:
        """Calcula m√©tricas abrangentes"""
        metrics = {}
        
        for i, tname in enumerate(target_names):
            yt = y_true[:, i]
            yp = y_pred[:, i]
            
            # M√©tricas b√°sicas
            rmse = float(np.sqrt(mean_squared_error(yt, yp)))
            mae = float(mean_absolute_error(yt, yp))
            r2 = float(r2_score(yt, yp))
            
            # MAPE robusto
            epsilon = 1e-8
            mape = np.mean(np.abs((yt - yp) / (np.abs(yt) + epsilon))) * 100
            
            # Correla√ß√£o
            correlation = float(np.corrcoef(yt, yp)[0, 1])
            
            # Vi√©s (bias)
            bias = float(np.mean(yp - yt))
            
            # M√©tricas de distribui√ß√£o
            std_ratio = float(np.std(yp) / np.std(yt))
            
            metrics[tname] = {
                "rmse": rmse,
                "mae": mae,
                "r2": r2,
                "mape": float(mape),
                "correlation": correlation,
                "bias": bias,
                "std_ratio": std_ratio,
                "prediction_mean": float(np.mean(yp)),
                "true_mean": float(np.mean(yt)),
                "prediction_std": float(np.std(yp)),
                "true_std": float(np.std(yt))
            }
        
        # M√©tricas globais
        metrics["global"] = {
            "rmse": float(np.sqrt(mean_squared_error(y_true, y_pred))),
            "mae": float(mean_absolute_error(y_true, y_pred)),
            "r2": float(r2_score(y_true, y_pred)),
            "explained_variance": float(np.var(y_pred) / np.var(y_true))
        }
        
        return metrics
    
    @staticmethod
    def analyze_residuals(y_true: np.ndarray, y_pred: np.ndarray, 
                         target_names: List[str], save_path: str):
        """Analisa e plota res√≠duos"""
        fig, axes = plt.subplots(2, len(target_names), figsize=(5*len(target_names), 10))
        
        if len(target_names) == 1:
            axes = axes.reshape(-1, 1)
        
        for i, tname in enumerate(target_names):
            yt = y_true[:, i]
            yp = y_pred[:, i]
            residuals = yp - yt
            
            # Plot res√≠duos vs preditos
            axes[0, i].scatter(yp, residuals, alpha=0.5)
            axes[0, i].axhline(y=0, color='r', linestyle='--')
            axes[0, i].set_xlabel('Predicted')
            axes[0, i].set_ylabel('Residuals')
            axes[0, i].set_title(f'{tname} - Residuals vs Predicted')
            
            # Histograma de res√≠duos
            axes[1, i].hist(residuals, bins=50, alpha=0.7)
            axes[1, i].axvline(x=0, color='r', linestyle='--')
            axes[1, i].set_xlabel('Residuals')
            axes[1, i].set_ylabel('Frequency')
            axes[1, i].set_title(f'{tname} - Residual Distribution')
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"üìä An√°lise de res√≠duos salva em: {save_path}")

# ============================================================
# PIPELINE DE TREINO AVAN√áADO
# ============================================================

def advanced_training_pipeline(df: pd.DataFrame, horizons: List[int], 
                             model_types: List[str], run_id: str):
    """Pipeline completo de treino avan√ßado"""
    
    # 1. Sele√ß√£o de targets e features
    target_cols, feature_cols = select_targets_and_features(df)
    
    # 2. Engenharia avan√ßada de features
    if AdvancedTrainConfig.USE_FEATURE_ENGINEERING:
        df_engineered = create_advanced_features(df, target_cols)
    else:
        df_engineered = df
    
    # 3. Preparar dados
    df_clean = prepare_data_for_training(df_engineered, feature_cols, target_cols)
    
    results = {}
    
    for model_type in model_types:
        logger.info(f"\nüéØ Treinando {model_type.upper()}")
        results[model_type] = {}
        
        for horizon in horizons:
            try:
                result = train_advanced_model(
                    df_clean, feature_cols, target_cols, 
                    horizon, model_type, run_id
                )
                
                if result:
                    results[model_type][horizon] = result
                    
            except Exception as e:
                logger.error(f"‚ùå Erro em {model_type} H{horizon}: {str(e)}")
                logger.debug(traceback.format_exc())
    
    return results

def train_advanced_model(df: pd.DataFrame, feature_cols: List[str], 
                        target_cols: List[str], horizon: int, 
                        model_type: str, run_id: str) -> Optional[Dict[str, Any]]:
    """Treina modelo individual com t√©cnicas avan√ßadas"""
    
    logger.info(f"üöÄ Iniciando treino {model_type.upper()} H{horizon}h")
    
    # Criar sequ√™ncias
    X, y = create_sequences(
        df, feature_cols, target_cols, 
        AdvancedTrainConfig.MAIN_LOOKBACK, horizon
    )
    
    if len(X) < AdvancedTrainConfig.MIN_SAMPLES:
        logger.warning(f"‚ö†Ô∏è Amostras insuficientes: {len(X)}")
        return None
    
    # Split dos dados
    (X_train, y_train), (X_val, y_val), (X_test, y_test), sc_X, sc_y = prepare_data(
        X, y, AdvancedTrainConfig.VAL_SPLIT, AdvancedTrainConfig.TEST_SPLIT
    )
    
    # Otimiza√ß√£o de hiperpar√¢metros
    if AdvancedTrainConfig.TUNE_HYPERPARAMETERS:
        best_hps = tune_hyperparameters(X_train, y_train, X_val, y_val, 
                                      (AdvancedTrainConfig.MAIN_LOOKBACK, len(feature_cols)), 
                                      len(target_cols))
        model = build_model_with_hps(model_type, best_hps, 
                                   (AdvancedTrainConfig.MAIN_LOOKBACK, len(feature_cols)), 
                                   len(target_cols))
    else:
        model = build_advanced_model(model_type, 
                                   (AdvancedTrainConfig.MAIN_LOOKBACK, len(feature_cols)), 
                                   len(target_cols))
    
    # Callbacks avan√ßados
    callbacks = create_advanced_callbacks(model_type, horizon, run_id)
    
    # Treino
    history = model.fit(
        X_train, y_train,
        epochs=AdvancedTrainConfig.MAX_EPOCHS,
        batch_size=AdvancedTrainConfig.BATCH_SIZE,
        validation_data=(X_val, y_val),
        callbacks=callbacks,
        shuffle=False,
        verbose=1
    )
    
    # Avalia√ß√£o
    y_pred_s = model.predict(X_test, verbose=0)
    y_pred = sc_y.inverse_transform(y_pred_s)
    y_true = sc_y.inverse_transform(y_test)
    
    # M√©tricas
    metrics = AdvancedMetrics.compute_comprehensive_metrics(y_true, y_pred, target_cols)
    
    # An√°lise de res√≠duos
    residuals_path = f"results/training/residuals_{model_type}_h{horizon}_{run_id}.png"
    AdvancedMetrics.analyze_residuals(y_true, y_pred, target_cols, residuals_path)
    
    logger.info(f"‚úÖ {model_type.upper()} H{horizon}h finalizado - "
               f"RMSE: {metrics['global']['rmse']:.3f}")
    
    return {
        'model': model,
        'history': history.history,
        'metrics': metrics,
        'scaler_X': sc_X,
        'scaler_Y': sc_y,
        'feature_cols': feature_cols,
        'target_cols': target_cols,
        'horizon': horizon
    }

# ============================================================
# FUN√á√ïES AUXILIARES COMPLETAS
# ============================================================

def select_targets_and_features(df: pd.DataFrame) -> Tuple[List[str], List[str]]:
    """Sele√ß√£o robusta de targets e features"""
    # Implementa√ß√£o similar √† vers√£o anterior, mas mais robusta
    cols_lower = {c.lower(): c for c in df.columns}
    
    # Targets priorit√°rios
    target_candidates = {
        'speed': ['speed', 'flow_speed', 'v_sw', 'vx', 'vp'],
        'bz_gse': ['bz_gse', 'bz', 'bz_gsm', 'bzgse'],
        'density': ['density', 'n_p', 'np', 'proton_density', 'n']
    }
    
    target_cols = []
    for target_name, candidates in target_candidates.items():
        for cand in candidates:
            if cand in cols_lower:
                target_cols.append(cols_lower[cand])
                logger.info(f"üéØ Target '{target_name}' identificado como: {cols_lower[cand]}")
                break
    
    if not target_cols:
        raise ValueError("‚ùå Nenhum target identificado")
    
    # Features: todas as num√©ricas
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    feature_cols = list(dict.fromkeys(target_cols + numeric_cols))
    
    logger.info(f"üìä {len(target_cols)} targets, {len(feature_cols)} features")
    return target_cols, feature_cols

def prepare_data_for_training(df: pd.DataFrame, feature_cols: List[str], 
                            target_cols: List[str]) -> pd.DataFrame:
    """Prepara dados finais para treino"""
    required_cols = list(set(feature_cols + target_cols + ['timestamp']))
    df_clean = df[required_cols].dropna()
    
    if len(df_clean) < AdvancedTrainConfig.MIN_SAMPLES:
        raise ValueError(f"‚ùå Dados insuficientes ap√≥s limpeza: {len(df_clean)}")
    
    logger.info(f"üßπ Dados preparados: {len(df_clean)} amostras")
    return df_clean

# ============================================================
# FUN√á√ÉO MAIN ATUALIZADA
# ============================================================

def main(horizons: List[int] = None, model_types: List[str] = None):
    """Fun√ß√£o principal atualizada"""
    logger.info("üöÄ HAC OMNI v6.0 - Iniciando treinamento avan√ßado")
    start_time = datetime.utcnow()
    
    # Configura√ß√£o
    setup_advanced_gpu()
    horizons = horizons or [1, 3, 6, 12, 24, 48]
    model_types = model_types or ['lstm', 'gru', 'hybrid']
    
    # Diret√≥rios
    os.makedirs("models/hac_omni_v6", exist_ok=True)
    os.makedirs("results/training/v6", exist_ok=True)
    os.makedirs("results/plots/v6", exist_ok=True)
    
    # Carregar dados
    df = load_and_validate_omni_data()
    
    # Pipeline de treino
    run_id = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    results = advanced_training_pipeline(df, horizons, model_types, run_id)
    
    # Salvar resultados
    save_advanced_results(results, run_id)
    
    exec_time = datetime.utcnow() - start_time
    logger.info(f"‚úÖ Treinamento completo! Tempo total: {exec_time}")

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="HAC OMNI v6.0 - Treino Avan√ßado")
    parser.add_argument("--horizons", nargs="+", type=int, 
                       help="Horizontes em horas (ex: 1 3 6 12 24 48)")
    parser.add_argument("--models", nargs="+", type=str,
                       help="Modelos (lstm, gru, hybrid, attention)")
    
    args = parser.parse_args()
    
    main(args.horizons, args.models)
```
